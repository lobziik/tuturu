# tuturu Cover Website Configuration
# Serves legitimate-looking static website as cover traffic
# Helps evade DPI active probing by appearing as normal website
# Listens on internal port 8443 (receives traffic routed from stream module port 443)

# Redirect HTTP to HTTPS (default server for HTTP)
server {
    listen 80 default_server;
    listen [::]:80 default_server;
    server_name _;

    # Redirect all HTTP requests to HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}

# HTTPS server for cover website (default server for HTTPS)
server {
    listen 8443 ssl http2 default_server;
    listen [::]:8443 ssl http2 default_server;
    server_name _;

    # SSL configuration
    # Uses base domain certificate (yourdomain.com)
    ssl_certificate /etc/ssl/turn/${DOMAIN}/fullchain.pem;
    ssl_certificate_key /etc/ssl/turn/${DOMAIN}/privkey.pem;

    # TLS protocols and ciphers (modern, secure)
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;

    # SSL session caching
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;

    # OCSP stapling
    ssl_stapling on;
    ssl_stapling_verify on;

    # Security headers
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;

    # Document root for static files
    root /var/www/html;
    index index.html index.htm;

    # Serve static files
    location / {
        try_files $uri $uri/ =404;

        # Cache static assets
        expires 7d;
        add_header Cache-Control "public, immutable";
    }

    # Favicon (prevent 404 logs)
    location = /favicon.ico {
        log_not_found off;
        access_log off;
        try_files /favicon.ico =204;
    }

    # Robots.txt (allow indexing of cover site to appear legitimate)
    location = /robots.txt {
        access_log off;
        return 200 "User-agent: *\nAllow: /\n";
    }

    # Error pages
    error_page 404 /404.html;
    error_page 500 502 503 504 /50x.html;

    location = /404.html {
        internal;
    }

    location = /50x.html {
        internal;
    }
}
